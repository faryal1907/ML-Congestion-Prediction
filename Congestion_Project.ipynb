{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c899ae-7475-4526-94f6-9b885f06abf1",
   "metadata": {},
   "source": [
    "# **CS 245 Machine Learning Course Project: Micro-Scale Road Congestion Prediction in Lahore**\n",
    "\n",
    "**Group Members:** [Faryal Khan, 465125], [Wania Mateen, ID], [Haleema Imran, ID]  \n",
    "**Theme:** T1: Urban Intelligence & City Futures  \n",
    "**Problem:** Predict 10-15 min horizon congestion hotspots on Lahore roads using accidents, mobility, weather, and OSM data.  \n",
    "**Stakeholders:** Lahore Traffic Police, commuters.  \n",
    "\n",
    "This notebook implements the end-to-end pipeline: data curation, ML models, evaluation, and PoC prep. Run top-to-bottom after populating `data/raw/`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23002d-b032-4f4b-87ec-08dad8b16140",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68d7851-627b-4b67-baa6-a6d9866f9655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete! Paths ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Paths (team: adjust if needed)\n",
    "raw_path = \"data/raw/\"\n",
    "processed_path = \"data/processed/\"\n",
    "models_path = \"models/\"\n",
    "os.makedirs(processed_path, exist_ok=True)\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete! Paths ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e7743c-ab68-4b25-a001-af6c6bf1e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RTA Accidents Dataset Exploration ===\n",
      "Shape: (46189, 25) (rows, cols)\n",
      "Columns: ['EcYear', 'EcNumber', 'CallTime', 'EmergencyArea', 'TotalPatientsInEmergency', 'Gender', 'Age', 'HospitalName', 'Reason', 'responsetime', 'EducationTitle', 'InjuryType', 'Cause', 'PatientStatus', 'BicycleInvovled', 'BikesInvolved', 'BusesInvolved', 'CarsInvolved', 'CartInvovled', 'RickshawsInvolved', 'TractorInvovled', 'TrainsInvovled', 'TrucksInvolved', 'VansInvolved', 'OthersInvolved']\n",
      "Data types:\n",
      "EcYear                              object\n",
      "EcNumber                            object\n",
      "CallTime                    datetime64[ns]\n",
      "EmergencyArea                       object\n",
      "TotalPatientsInEmergency            object\n",
      "Gender                              object\n",
      "Age                                float64\n",
      "HospitalName                        object\n",
      "Reason                              object\n",
      "responsetime                       float64\n",
      "EducationTitle                      object\n",
      "InjuryType                          object\n",
      "Cause                               object\n",
      "PatientStatus                       object\n",
      "BicycleInvovled                    float64\n",
      "BikesInvolved                      float64\n",
      "BusesInvolved                      float64\n",
      "CarsInvolved                       float64\n",
      "CartInvovled                       float64\n",
      "RickshawsInvolved                  float64\n",
      "TractorInvovled                    float64\n",
      "TrainsInvovled                     float64\n",
      "TrucksInvolved                     float64\n",
      "VansInvolved                       float64\n",
      "OthersInvolved                     float64\n",
      "dtype: object\n",
      "Missing values per column:\n",
      "EcYear                          0\n",
      "EcNumber                     7211\n",
      "CallTime                     5956\n",
      "EmergencyArea                   1\n",
      "TotalPatientsInEmergency        0\n",
      "Gender                          1\n",
      "Age                             1\n",
      "HospitalName                21950\n",
      "Reason                          1\n",
      "responsetime                    5\n",
      "EducationTitle                  1\n",
      "InjuryType                      2\n",
      "Cause                           2\n",
      "PatientStatus                   2\n",
      "BicycleInvovled                 2\n",
      "BikesInvolved                   2\n",
      "BusesInvolved                   2\n",
      "CarsInvolved                    2\n",
      "CartInvovled                    2\n",
      "RickshawsInvolved               2\n",
      "TractorInvovled                 2\n",
      "TrainsInvovled                  2\n",
      "TrucksInvolved                  2\n",
      "VansInvolved                    2\n",
      "OthersInvolved                  2\n",
      "dtype: int64\n",
      "\n",
      "First 3 rows:\n",
      "   EcYear EcNumber            CallTime                                        EmergencyArea TotalPatientsInEmergency Gender   Age HospitalName                 Reason  responsetime EducationTitle       InjuryType       Cause     PatientStatus  BicycleInvovled  BikesInvolved  BusesInvolved  CarsInvolved  CartInvovled  RickshawsInvolved  TractorInvovled  TrainsInvovled  TrucksInvolved  VansInvolved  OthersInvolved\n",
      "0   2020    31486 2020-12-31 22:41:47                        NEAR APS SCHOOL FORT ROAD RWP                        1   Male  27.0          BBH              Bike Slip          10.0   Intermediate            Minor  Over Speed  Alive & unstable              0.0            1.0            0.0           0.0           0.0                0.0              0.0             0.0             0.0           0.0             0.0\n",
      "1   2020    31485 2020-12-31 22:25:00   Infront of Daig.com, Near Dha gate 2,  gt road rwp                        1   Male  20.0          NaN       Car hit Footpath          12.0      Illetrate            Minor  Over Speed    Alive & stable              0.0            0.0            0.0           1.0           0.0                0.0              0.0             0.0             0.0           0.0             0.0\n",
      "2   2020    31483 2020-12-31 21:54:59  Muhammadi chowk arshad bakery khyaban e sirsyed rwp                        1   Male  48.0          BBH  Rickshaw hit with Car          10.0      Illetrate  Single Fracture  Over Speed  Alive & unstable              0.0            0.0            0.0           1.0           0.0                1.0              0.0             0.0             0.0           0.0             0.0\n",
      "\n",
      "Date-like columns sample (first 5 unique dates):\n",
      "\n",
      "CallTime: <DatetimeArray>\n",
      "['2020-12-31 22:41:47', '2020-12-31 22:25:00', '2020-12-31 21:54:59',\n",
      " '2020-12-31 21:24:22', '2020-12-31 21:03:49']\n",
      "Length: 5, dtype: datetime64[ns]\n",
      "responsetime: [10. 12.  5.  6.  4.]\n",
      "\n",
      "Lat/Lon sample (first 5):\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== RTA Accidents Dataset Exploration ===\")\n",
    "rta_path = \"data/raw/RTA Data 2020 to July 2023.xlsx\"\n",
    "rta = pd.read_excel(rta_path)\n",
    "print(f\"Shape: {rta.shape} (rows, cols)\")\n",
    "print(f\"Columns: {rta.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{rta.dtypes}\")\n",
    "print(f\"Missing values per column:\\n{rta.isnull().sum()}\")\n",
    "print(\"\\nFirst 3 rows:\\n\", rta.head(3).to_string())\n",
    "print(\"\\nDate-like columns sample (first 5 unique dates):\\n\")\n",
    "date_cols = [col for col in rta.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "for col in date_cols[:3]:  # Top 3 potential date/time cols\n",
    "    print(f\"{col}: {rta[col].dropna().unique()[:5]}\")\n",
    "print(\"\\nLat/Lon sample (first 5):\\n\", rta[[col for col in rta.columns if 'lat' in col.lower() or 'lon' in col.lower()]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe97e60-f5f2-47ef-b0e2-797fbe3c1309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading PK Mobility Files ===\n",
      "Concat shape: (20404, 16)\n",
      "Columns: ['country_region_code', 'country_region', 'sub_region_1', 'sub_region_2', 'metro_area', 'iso_3166_2_code', 'census_fips_code', 'place_id', 'date', 'retail_and_recreation_percent_change_from_baseline', 'grocery_and_pharmacy_percent_change_from_baseline', 'parks_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline', 'workplaces_percent_change_from_baseline', 'residential_percent_change_from_baseline', 'datetime']\n",
      "Unique sub_region_1: [nan 'Azad Jammu and Kashmir' 'Balochistan'\n",
      " 'Federally Administered Tribal Area' 'Gilgit-Baltistan'\n",
      " 'Islamabad Capital Territory' 'Khyber Pakhtunkhwa' 'Punjab' 'Sindh']\n",
      "Unique sub_region_2: [nan]\n",
      "\n",
      "Sample:\n",
      "   sub_region_1  sub_region_2       date  workplaces_percent_change_from_baseline\n",
      "0          NaN           NaN 2020-02-15                                      2.0\n",
      "1          NaN           NaN 2020-02-16                                      2.0\n",
      "2          NaN           NaN 2020-02-17                                      3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Loading PK Mobility Files ===\")\n",
    "files = [\n",
    "    \"data/raw/2020_PK_Region_Mobility_Report.xlsx\",\n",
    "    \"data/raw/2021_PK_Region_Mobility_Report.xlsx\",\n",
    "    \"data/raw/2022_PK_Region_Mobility_Report.xlsx\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_excel(file)\n",
    "    df['datetime'] = pd.to_datetime(df['date'])\n",
    "    dfs.append(df)\n",
    "mob_pk = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Concat shape: {mob_pk.shape}\")\n",
    "print(\"Columns:\", mob_pk.columns.tolist())\n",
    "print(\"Unique sub_region_1:\", mob_pk['sub_region_1'].unique())\n",
    "print(\"Unique sub_region_2:\", mob_pk['sub_region_2'].unique()[:10])  # Top 10\n",
    "print(\"\\nSample:\\n\", mob_pk.head(3)[['sub_region_1', 'sub_region_2', 'date', 'workplaces_percent_change_from_baseline']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ee6c38-195d-4303-bf6a-0c087e04a3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weather Datasets Exploration ===\n",
      "Combined shape: (34245, 27)\n",
      "Columns: ['date', 'year', 'month', 'day', 'dayofweek', 'is_weekend', 'season', 'city', 'region', 'latitude', 'longitude', 'elevation', 'tmin', 'tmax', 'tavg', 'prcp', 'wspd', 'humidity', 'pressure', 'dew_point', 'cloud_cover', 'visibility', 'temp_range', 'is_hot_day', 'is_cold_day', 'rainfall_intensity', 'wind_category']\n",
      "Data types:\n",
      "date                   object\n",
      "year                    int64\n",
      "month                   int64\n",
      "day                     int64\n",
      "dayofweek               int64\n",
      "is_weekend              int64\n",
      "season                 object\n",
      "city                   object\n",
      "region                 object\n",
      "latitude              float64\n",
      "longitude             float64\n",
      "elevation             float64\n",
      "tmin                  float64\n",
      "tmax                  float64\n",
      "tavg                  float64\n",
      "prcp                  float64\n",
      "wspd                  float64\n",
      "humidity              float64\n",
      "pressure              float64\n",
      "dew_point             float64\n",
      "cloud_cover           float64\n",
      "visibility            float64\n",
      "temp_range            float64\n",
      "is_hot_day              int64\n",
      "is_cold_day             int64\n",
      "rainfall_intensity     object\n",
      "wind_category          object\n",
      "dtype: object\n",
      "Missing values per column:\n",
      "date                      0\n",
      "year                      0\n",
      "month                     0\n",
      "day                       0\n",
      "dayofweek                 0\n",
      "is_weekend                0\n",
      "season                    0\n",
      "city                      0\n",
      "region                    0\n",
      "latitude                  0\n",
      "longitude                 0\n",
      "elevation                 0\n",
      "tmin                      0\n",
      "tmax                      0\n",
      "tavg                      0\n",
      "prcp                      0\n",
      "wspd                      0\n",
      "humidity                  0\n",
      "pressure                  0\n",
      "dew_point                 0\n",
      "cloud_cover               0\n",
      "visibility            34245\n",
      "temp_range                0\n",
      "is_hot_day                0\n",
      "is_cold_day               0\n",
      "rainfall_intensity        0\n",
      "wind_category             0\n",
      "dtype: int64\n",
      "\n",
      "First 3 rows:\n",
      "        date  year  month  day  dayofweek  is_weekend  season       city  region  latitude  longitude  elevation  tmin  tmax  tavg  prcp  wspd  humidity  pressure  dew_point  cloud_cover  visibility  temp_range  is_hot_day  is_cold_day rainfall_intensity wind_category\n",
      "0  1/1/2000  2000      1    1          5           1  Winter  Islamabad  Punjab   33.6844    73.0479      540.0   3.1  17.2   9.4   0.0   6.0      76.0    1018.9        4.8          0.0         NaN        14.1           0            0               none         windy\n",
      "1  1/2/2000  2000      1    2          6           1  Winter  Islamabad  Punjab   33.6844    73.0479      540.0   2.9  17.4   9.3   0.0   6.1      77.0    1018.6        4.8          0.0         NaN        14.5           0            0               none         windy\n",
      "2  1/3/2000  2000      1    3          0           0  Winter  Islamabad  Punjab   33.6844    73.0479      540.0   3.1  17.7   9.6   0.0   6.2      76.0    1016.8        5.0          0.0         NaN        14.6           0            0               none         windy\n",
      "\n",
      "Lahore sample (first 5 rows):\n",
      "\n",
      "           date  year  month  day  dayofweek  is_weekend  season    city  region  latitude  longitude  elevation  tmin  tmax  tavg  prcp  wspd  humidity  pressure  dew_point  cloud_cover  visibility  temp_range  is_hot_day  is_cold_day rainfall_intensity wind_category\n",
      "14976  1/1/2016  2016      1    1          4           0  Winter  Lahore  Punjab   31.5497    74.3436      217.0   9.9  20.9  14.5   0.0   6.6      64.0    1021.0        7.3         46.0         NaN        11.0           0            0               none         windy\n",
      "14977  1/2/2016  2016      1    2          5           1  Winter  Lahore  Punjab   31.5497    74.3436      217.0   6.2  20.6  13.1   0.0  10.8      69.0    1021.9        6.8         37.0         NaN        14.4           0            0               none         windy\n",
      "14978  1/3/2016  2016      1    3          6           1  Winter  Lahore  Punjab   31.5497    74.3436      217.0   7.6  21.6  14.2   0.0  10.4      65.0    1018.7        6.9         34.0         NaN        14.0           0            0               none         windy\n",
      "14979  1/4/2016  2016      1    4          0           0  Winter  Lahore  Punjab   31.5497    74.3436      217.0   9.5  20.9  14.7   0.6   7.8      73.0    1018.7        9.5         63.0         NaN        11.4           0            0              light         windy\n",
      "14980  1/5/2016  2016      1    5          1           0  Winter  Lahore  Punjab   31.5497    74.3436      217.0   8.9  21.9  14.9   0.0   9.6      77.0    1018.6       10.3         27.0         NaN        13.0           0            0               none         windy\n",
      "\n",
      "Unique cities: ['Islamabad' 'Karachi' 'Lahore' 'Peshawar' 'Quetta' 'Gilgit']\n",
      "Date range: 1/1/2000 to 9/9/2024\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Weather Datasets Exploration ===\")\n",
    "w1_path = \"data/raw/pakistan_weather_2000_2024.csv\"\n",
    "w2_path = \"data/raw/pakistan_weather_data-Sep2024-Oct2025.csv\"\n",
    "w1 = pd.read_csv(w1_path)\n",
    "w2 = pd.read_csv(w2_path)\n",
    "weather = pd.concat([w1, w2], ignore_index=True)\n",
    "print(f\"Combined shape: {weather.shape}\")\n",
    "print(f\"Columns: {weather.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{weather.dtypes}\")\n",
    "print(f\"Missing values per column:\\n{weather.isnull().sum()}\")\n",
    "print(\"\\nFirst 3 rows:\\n\", weather.head(3).to_string())\n",
    "print(\"\\nLahore sample (first 5 rows):\\n\")\n",
    "lahore_sample = weather[weather['city'].str.contains('Lahore', na=False, case=False)].head()\n",
    "print(lahore_sample.to_string())\n",
    "print(f\"\\nUnique cities: {weather['city'].unique()[:10]}\")\n",
    "print(f\"Date range: {weather['date'].min()} to {weather['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45885894-0a27-4af1-9fc2-2cf1e6197a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Roads Dataset Exploration (From Local PBF - Offline) ===\n",
      "Extracted 0 road segments from PBF.\n",
      "No roads found—widen bbox or check PBF.\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Roads Dataset Exploration (From Local PBF - Offline) ===\")\n",
    "\n",
    "import osmium\n",
    "from shapely.geometry import LineString\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# Lahore bbox for filtering (min_lat, min_lon, max_lat, max_lon)\n",
    "lahore_bounds = (31.4, 74.1, 31.7, 74.5)\n",
    "\n",
    "# Handler to extract nodes and ways (roads) in bbox\n",
    "class LahoreHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nodes = {}  # Store node coords\n",
    "        self.ways = []  # Store roads\n",
    "\n",
    "    def node(self, n):\n",
    "        lat, lon = n.location.lat, n.location.lon\n",
    "        if lahore_bounds[0] <= lat <= lahore_bounds[2] and lahore_bounds[1] <= lon <= lahore_bounds[3]:\n",
    "            self.nodes[n.id] = (lat, lon)\n",
    "\n",
    "    def way(self, w):\n",
    "        if 'highway' in w.tags:  # Only roads\n",
    "            coords = []\n",
    "            for nd in w.nodes:\n",
    "                if nd in self.nodes:\n",
    "                    coords.append(self.nodes[nd])\n",
    "            if len(coords) >= 2:  # Valid line\n",
    "                geometry = LineString(coords)\n",
    "                # Approx length in km (haversine simple for equatorial approx; Lahore is ~31°N)\n",
    "                length = sum(np.sqrt((coords[i][1] - coords[i+1][1])**2 + ((coords[i][0] - coords[i+1][0]) * np.cos(np.radians(coords[i][0])) )**2 ) * 111 for i in range(len(coords)-1)) / 1000\n",
    "                self.ways.append({\n",
    "                    'highway': w.tags.get('highway', 'unknown'),\n",
    "                    'name': w.tags.get('name', 'unnamed'),\n",
    "                    'geometry': geometry,\n",
    "                    'length': length\n",
    "                })\n",
    "\n",
    "# Process PBF\n",
    "handler = LahoreHandler()\n",
    "handler.apply_file(\"data/raw/pakistan-251119.osm.pbf\")\n",
    "print(f\"Extracted {len(handler.ways)} road segments from PBF.\")\n",
    "\n",
    "if len(handler.ways) == 0:\n",
    "    print(\"No roads found—widen bbox or check PBF.\")\n",
    "else:\n",
    "    # Create GeoDataFrame with explicit geometry column\n",
    "    roads_gdf = gpd.GeoDataFrame(handler.ways, geometry='geometry', crs=\"EPSG:4326\")\n",
    "    roads_gdf[\"segment_id\"] = range(len(roads_gdf))\n",
    "\n",
    "    print(f\"Shape: {roads_gdf.shape} (road segments)\")\n",
    "    print(f\"Columns: {roads_gdf.columns.tolist()}\")\n",
    "    print(f\"Data types:\\n{roads_gdf.dtypes}\")\n",
    "    print(f\"Missing values per column:\\n{roads_gdf.isnull().sum()}\")\n",
    "    print(\"\\nFirst 3 segments:\\n\", roads_gdf.head(3)[['highway', 'name', 'length', 'geometry']].to_string())\n",
    "    print(f\"Road types (top 5): \\n{roads_gdf['highway'].value_counts().head()}\")\n",
    "    print(f\"Average length: {roads_gdf['length'].mean():.2f} km\")\n",
    "    print(f\"Total road length: {roads_gdf['length'].sum():.1f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8be6a47-4645-4737-97d9-df2ceca81101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHX restore enabled.\n",
      "=== Roads from Shapefile ===\n",
      "Shape: (79354, 3)\n",
      "Columns: ['geometry', 'segment_id', 'length']\n",
      "                                              geometry  segment_id  \\\n",
      "227  LINESTRING (74.29559 31.47191, 74.29662 31.47157)           0   \n",
      "228  LINESTRING (74.29684 31.47189, 74.29662 31.47157)           1   \n",
      "229  LINESTRING (74.29924 31.46942, 74.29917 31.469...           2   \n",
      "\n",
      "           length  \n",
      "227  1.083790e-06  \n",
      "228  3.834327e-07  \n",
      "229  3.594147e-07  \n",
      "Road types (top 5): \n",
      "No highway col\n",
      "Average length: 0.00 km\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import from_wkt  # Use shapely directly for single WKT\n",
    "\n",
    "print(\"SHX restore enabled.\")\n",
    "\n",
    "print(\"=== Roads from Shapefile ===\")\n",
    "roads_path = \"data/raw/hotosm_pak_roads_lines_shp.shp\"  # Confirm with os.listdir if needed\n",
    "roads_gdf = gpd.read_file(roads_path)\n",
    "\n",
    "# Filter Lahore (bbox polygon using shapely for single WKT)\n",
    "lahore_wkt = 'POLYGON((74.1 31.4, 74.5 31.4, 74.5 31.7, 74.1 31.7, 74.1 31.4))'\n",
    "lahore_poly = from_wkt(lahore_wkt)\n",
    "roads_gdf = roads_gdf[roads_gdf.intersects(lahore_poly)]\n",
    "roads_gdf[\"segment_id\"] = range(len(roads_gdf))\n",
    "if 'length' not in roads_gdf.columns:\n",
    "    roads_gdf[\"length\"] = roads_gdf.geometry.length / 1000  # km\n",
    "print(f\"Shape: {roads_gdf.shape}\")\n",
    "print(f\"Columns: {roads_gdf.columns.tolist()}\")\n",
    "print(roads_gdf.head(3))\n",
    "print(f\"Road types (top 5): \\n{roads_gdf['highway'].value_counts().head() if 'highway' in roads_gdf.columns else 'No highway col'}\")\n",
    "print(f\"Average length: {roads_gdf['length'].mean():.2f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dffc7f72-8f53-440d-a337-3a5f456eea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Roads from Shapefile (Warning Fixed) ===\n",
      "Full shape: (1077644, 2), Lahore filter shape: (79354, 5)\n",
      "Columns: ['geometry', 'length', 'segment_id', 'highway', 'name']\n",
      "Data types:\n",
      "geometry      geometry\n",
      "length         float64\n",
      "segment_id       int64\n",
      "highway         object\n",
      "name            object\n",
      "dtype: object\n",
      "Missing values per column:\n",
      "geometry      0\n",
      "length        0\n",
      "segment_id    0\n",
      "highway       0\n",
      "name          0\n",
      "dtype: int64\n",
      "\n",
      "First 3 segments:\n",
      "      highway     name    length                                                                                                                      geometry\n",
      "227  unknown  unnamed  0.104746                                                                             LINESTRING (74.29559 31.47191, 74.29662 31.47157)\n",
      "228  unknown  unnamed  0.040637                                                                             LINESTRING (74.29684 31.47189, 74.29662 31.47157)\n",
      "229  unknown  unnamed  0.038187  LINESTRING (74.29924 31.46942, 74.29917 31.46932, 74.29913 31.46928, 74.2991 31.46923, 74.29907 31.46917, 74.29905 31.46912)\n",
      "Road types (top 5): \n",
      "highway\n",
      "unknown    79354\n",
      "Name: count, dtype: int64\n",
      "Average length: 0.18 km\n",
      "Total road length: 14134.3 km\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import from_wkt\n",
    "import os\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'  # SHX restore\n",
    "\n",
    "print(\"=== Roads from Shapefile (Warning Fixed) ===\")\n",
    "roads_path = \"data/raw/hotosm_pak_roads_lines_shp.shp\"  # Confirm with os.listdir if needed\n",
    "roads_gdf = gpd.read_file(roads_path)\n",
    "\n",
    "# Set original CRS (assume WGS84 for OSM shapefile)\n",
    "roads_gdf = roads_gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "# Project to UTM for metric lengths (Lahore zone 43N)\n",
    "roads_gdf_projected = roads_gdf.to_crs(\"EPSG:32643\")\n",
    "roads_gdf_projected[\"length\"] = roads_gdf_projected.geometry.length / 1000  # Now in km\n",
    "roads_gdf = roads_gdf_projected.to_crs(\"EPSG:4326\")  # Back to WGS84\n",
    "\n",
    "# Filter Lahore (bbox polygon) and copy to avoid warning\n",
    "lahore_wkt = 'POLYGON((74.1 31.4, 74.5 31.4, 74.5 31.7, 74.1 31.7, 74.1 31.4))'\n",
    "lahore_poly = from_wkt(lahore_wkt)\n",
    "lahore_roads = roads_gdf[roads_gdf.intersects(lahore_poly)].copy()  # .copy() fixes warning\n",
    "\n",
    "# Safe assignments with .loc\n",
    "lahore_roads.loc[:, \"segment_id\"] = range(len(lahore_roads))\n",
    "if 'highway' not in lahore_roads.columns:\n",
    "    lahore_roads.loc[:, \"highway\"] = \"unknown\"\n",
    "if 'name' not in lahore_roads.columns:\n",
    "    lahore_roads.loc[:, \"name\"] = \"unnamed\"\n",
    "\n",
    "print(f\"Full shape: {roads_gdf.shape}, Lahore filter shape: {lahore_roads.shape}\")\n",
    "print(f\"Columns: {lahore_roads.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{lahore_roads.dtypes}\")\n",
    "print(f\"Missing values per column:\\n{lahore_roads.isnull().sum()}\")\n",
    "print(\"\\nFirst 3 segments:\\n\", lahore_roads.head(3)[['highway', 'name', 'length', 'geometry']].to_string())\n",
    "print(f\"Road types (top 5): \\n{lahore_roads['highway'].value_counts().head()}\")\n",
    "print(f\"Average length: {lahore_roads['length'].mean():.2f} km\")\n",
    "print(f\"Total road length: {lahore_roads['length'].sum():.1f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b054071-343b-4d73-b128-9fc4d28eb061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Accidents ===\n",
      "Cleaned Lahore shape: (40232, 5)\n",
      "             datetime  Latitude  Longitude  incident_count          severity\n",
      "0 2020-12-31 22:41:47     31.55      74.35               1  Alive & unstable\n",
      "1 2020-12-31 22:25:00     31.55      74.35               1    Alive & stable\n",
      "2 2020-12-31 21:54:59     31.55      74.35               1  Alive & unstable\n"
     ]
    }
   ],
   "source": [
    "# Clean Accidents\n",
    "print(\"=== Cleaning Accidents ===\")\n",
    "rta = pd.read_excel(\"data/raw/RTA Data 2020 to July 2023.xlsx\")\n",
    "\n",
    "# Basic clean: Drop high-missing cols, handle datetime\n",
    "rta = rta.dropna(subset=['CallTime'])  # Drop missing time\n",
    "rta['datetime'] = pd.to_datetime(rta['CallTime'])\n",
    "rta = rta[rta['datetime'].dt.year.between(2020, 2023)]  # Filter years\n",
    "rta['incident_count'] = rta['TotalPatientsInEmergency'].fillna(1)  # Proxy (min 1)\n",
    "rta['severity'] = rta['PatientStatus'# Clean Mobility\n",
    "print(\"=== Cleaning Mobility ===\")\n",
    "mob = pd.read_excel(\"data/raw/Global_Mobility_Report.xlsx\")\n",
    "mob['datetime'] = pd.to_datetime(mob['date'])\n",
    "mob = mob[mob['datetime'].dt.year.between(2020, 2023)]  # Filter years\n",
    "\n",
    "# Punjab proxy (no Lahore; use sub_region_1 = 'Punjab')\n",
    "mob_punjab = mob[\n",
    "    (mob['country_region'] == 'Pakistan') & \n",
    "    (mob['sub_region_1'] == 'Punjab')\n",
    "][['datetime', 'workplaces_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline']]\n",
    "\n",
    "# Resample to 15-min (ffill daily to hourly/15-min for alignment)\n",
    "mob_punjab = mob_punjab.set_index('datetime').resample('15T').ffill()\n",
    "mob_punjab['mobility_proxy'] = mob_punjab['workplaces_percent_change_from_baseline'].fillna(0)  # Proxy for traffic demand\n",
    "\n",
    "print(f\"Cleaned Punjab shape: {mob_punjab.shape}\")\n",
    "print(mob_punjab.head(3))\n",
    "mob_punjab.to_csv(\"data/processed/cleaned_mobility.csv\")].fillna('Unknown')  # Keep for later\n",
    "\n",
    "# Proxy geo: Simple dict for common areas (Lahore/Rawalpindi; expand if needed)\n",
    "geo_proxy = {\n",
    "    'BBH': (31.55, 74.35),  # Benazir Bhutto Hospital approx Lahore\n",
    "    'Near APS SCHOOL FORT ROAD RWP': (33.60, 73.05),  # Rawalpindi (drop if non-Lahore)\n",
    "    # Add more from unique EmergencyArea\n",
    "}\n",
    "rta['Latitude'] = rta['EmergencyArea'].map(lambda x: geo_proxy.get(x, (31.55, 74.35))[0])  # Default Lahore center\n",
    "rta['Longitude'] = rta['EmergencyArea'].map(lambda x: geo_proxy.get(x, (31.55, 74.35))[1])\n",
    "\n",
    "# Lahore filter (using proxy lat/lon)\n",
    "lahore_bounds = (31.4, 74.1, 31.7, 74.5)\n",
    "rta_lahore = rta[\n",
    "    (rta['Latitude'].between(lahore_bounds[0], lahore_bounds[2])) & \n",
    "    (rta['Longitude'].between(lahore_bounds[1], lahore_bounds[3]))\n",
    "][['datetime', 'Latitude', 'Longitude', 'incident_count', 'severity']]\n",
    "\n",
    "print(f\"Cleaned Lahore shape: {rta_lahore.shape}\")\n",
    "print(rta_lahore.head(3))\n",
    "rta_lahore.to_csv(\"data/processed/cleaned_accidents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8089d905-e4cc-4b93-944e-4bed3995138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning PK Mobility ===\n",
      "Cleaned Punjab shape: (93409, 3)\n",
      "Head:\n",
      "                      workplaces_percent_change_from_baseline  \\\n",
      "datetime                                                       \n",
      "2020-02-15 00:00:00                                      2.0   \n",
      "2020-02-15 00:15:00                                      2.0   \n",
      "2020-02-15 00:30:00                                      2.0   \n",
      "\n",
      "                     transit_stations_percent_change_from_baseline  \\\n",
      "datetime                                                             \n",
      "2020-02-15 00:00:00                                            4.0   \n",
      "2020-02-15 00:15:00                                            4.0   \n",
      "2020-02-15 00:30:00                                            4.0   \n",
      "\n",
      "                     mobility_proxy  \n",
      "datetime                             \n",
      "2020-02-15 00:00:00             2.0  \n",
      "2020-02-15 00:15:00             2.0  \n",
      "2020-02-15 00:30:00             2.0  \n",
      "Mobility cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Clean PK Mobility (Based on Your Output)\n",
    "print(\"=== Cleaning PK Mobility ===\")\n",
    "files = [\n",
    "    \"data/raw/2020_PK_Region_Mobility_Report.xlsx\",\n",
    "    \"data/raw/2021_PK_Region_Mobility_Re# Clean Weather\n",
    "print(\"=== Cleaning Weather ===\")\n",
    "w1 = pd.read_csv(\"data/raw/pakistan_weather_2000_2024.csv\")\n",
    "w2 = pd.read_csv(\"data/raw/pakistan_weather_data-Sep2024-Oct2025.csv\")\n",
    "w1['datetime'] = pd.to_datetime(w1['date'])\n",
    "w2['datetime'] = pd.to_datetime(w2['date'])\n",
    "weather = pd.concat([w1, w2], ignore_index=True)\n",
    "\n",
    "# Lahore filter & year range\n",
    "weather_lahore = weather[\n",
    "    (weather['city'] == 'Lahore') & \n",
    "    (weather['datetime'].dt.year.between(2020, 2023))\n",
    "][['datetime', 'tavg', 'prcp', 'humidity', 'latitude', 'longitude']]\n",
    "\n",
    "# Resample to 15-min (interpolate for missing)\n",
    "weather_lahore = weather_lahore.set_index('datetime').resample('15T').interpolate(method='linear')\n",
    "weather_lahore['temperature'] = weather_lahore['tavg']\n",
    "weather_lahore['precipitation'] = weather_lahore['prcp']\n",
    "\n",
    "# Engineer lag for 10-15 min horizon\n",
    "weather_lahore['precip_lag'] = weather_lahore['precipitation'].shift(4)  # ~1 hour lag (4*15min)\n",
    "\n",
    "print(f\"Cleaned Lahore shape: {weather_lahore.shape}\")\n",
    "print(\"Head:\\n\", weather_lahore.head(3))\n",
    "weather_lahore.to_csv(\"data/processed/cleaned_weather.csv\", index=False)\n",
    "print(\"Weather cleaned!\")port.xlsx\",\n",
    "    \"data/raw/2022_PK_Region_Mobility_Report.xlsx\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_excel(file)\n",
    "    df['datetime'] = pd.to_datetime(df['date'])\n",
    "    dfs.append(df)\n",
    "mob_pk = pd.concat(dfs, ignore_index=True)\n",
    "mob_pk = mob_pk[mob_pk['datetime'].dt.year.between(2020, 2023)]  # Filter years\n",
    "\n",
    "# Punjab filter (from your uniques)\n",
    "mob_punjab = mob_pk[\n",
    "    (mob_pk['sub_region_1'] == 'Punjab')  # Exact match\n",
    "][['datetime', 'workplaces_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline']]\n",
    "\n",
    "# Resample to 15-min (ffill for alignment with other data)\n",
    "mob_punjab = mob_punjab.set_index('datetime').resample('15T').ffill()\n",
    "mob_punjab['mobility_proxy'] = mob_punjab['workplaces_percent_change_from_baseline'].fillna(0)  # Proxy for traffic demand\n",
    "\n",
    "print(f\"Cleaned Punjab shape: {mob_punjab.shape}\")\n",
    "print(\"Head:\\n\", mob_punjab.head(3))\n",
    "mob_punjab.to_csv(\"data/processed/cleaned_mobility.csv\", index=False)\n",
    "print(\"Mobility cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4fadd3f-adb6-438a-a2f3-12bbfc4ace76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Weather ===\n",
      "Lag NaN filled.\n",
      "Cleaned Lahore shape: (140161, 8)\n",
      "Head:\n",
      "                          tavg  prcp   humidity  latitude  longitude  \\\n",
      "datetime                                                              \n",
      "2020-01-01 00:00:00  6.200000   0.0  90.000000   31.5497    74.3436   \n",
      "2020-01-01 00:15:00  6.234375   0.0  89.947917   31.5497    74.3436   \n",
      "2020-01-01 00:30:00  6.268750   0.0  89.895833   31.5497    74.3436   \n",
      "\n",
      "                     temperature  precipitation  precip_lag  \n",
      "datetime                                                     \n",
      "2020-01-01 00:00:00     6.200000            0.0         0.0  \n",
      "2020-01-01 00:15:00     6.234375            0.0         0.0  \n",
      "2020-01-01 00:30:00     6.268750            0.0         0.0  \n",
      "Weather cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Clean Weather (Fixed Datetime)\n",
    "print(\"=== Cleaning Weather ===\")\n",
    "w1 = pd.read_csv(\"data/raw/pakistan_weather_2000_2024.csv\")\n",
    "w2 = pd.read_csv(\"data/raw/pakistan_weather_data-Sep2024-Oct2025.csv\")\n",
    "w1['datetime'] = pd.to_datetime(w1['date'], errors='coerce')\n",
    "w2['datetime'] = pd.to_datetime(w2['date'], errors='coerce')\n",
    "weather = pd.concat([w1, w2], ignore_index=True)\n",
    "\n",
    "# Re-apply datetime after concat (ensures consistency)\n",
    "weather['datetime'] = pd.to_datetime(weather['date'], errors='coerce')\n",
    "weather = weather.dropna(subset=['datetime'])  # Drop bad dates\n",
    "\n",
    "# Lahore filter & year range (now .dt works)\n",
    "weather_lahore = weather[\n",
    "    (weather['city'] == 'Lahore') & \n",
    "    (weather['datetime'].dt.year.between(2020, 2023))\n",
    "][['datetime', 'tavg', 'prcp', 'humidity', 'latitude', 'longitude']]\n",
    "\n",
    "# Resample to 15-min (interpolate for missing)\n",
    "weather_lahore = weather_lahore.set_index('datetime').resample('15T').interpolate(method='linear')\n",
    "weather_lahore['temperature'] = weather_lahore['tavg']\n",
    "weather_lahore['precipitation'] = weather_lahore['prcp']\n",
    "\n",
    "# Engineer lag for 10-15 min horizon\n",
    "weather_lahore['precip_lag'] = weather_lahore['precipitation'].shift(4)  # ~1 hour lag\n",
    "# Optional: Ffill lag NaN (propagate first value)\n",
    "weather_lahore['precip_lag'] = weather_lahore['precip_lag'].fillna(weather_lahore['precipitation'].iloc[0])\n",
    "print(\"Lag NaN filled.\")\n",
    "weather_lahore.to_csv(\"data/processed/cleaned_weather.csv\", index=True)  # Re-save\n",
    "\n",
    "print(f\"Cleaned Lahore shape: {weather_lahore.shape}\")\n",
    "print(\"Head:\\n\", weather_lahore.head(3))\n",
    "weather_lahore.to_csv(\"data/processed/cleaned_weather.csv\", index=False)\n",
    "\n",
    "print(\"Weather cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1029191f-f4b9-404e-8e7a-179138404240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Roads ===\n",
      "Full shape: (1077644, 2), Lahore shape: (79354, 5)\n",
      "Head:\n",
      "      highway     name    length  \\\n",
      "227  unknown  unnamed  0.104746   \n",
      "228  unknown  unnamed  0.040637   \n",
      "229  unknown  unnamed  0.038187   \n",
      "\n",
      "                                              geometry  \n",
      "227  LINESTRING (74.29559 31.47191, 74.29662 31.47157)  \n",
      "228  LINESTRING (74.29684 31.47189, 74.29662 31.47157)  \n",
      "229  LINESTRING (74.29924 31.46942, 74.29917 31.469...  \n",
      "Roads cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Clean Roads\n",
    "print(\"=== Cleaning Roads ===\")\n",
    "import geopandas as gpd\n",
    "from shapely import from_wkt  # Missing import\n",
    "\n",
    "roads_gdf = gpd.read_file(\"data/raw/hotosm_pak_roads_lines_shp.shp\")\n",
    "roads_gdf = roads_gdf.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "\n",
    "# Project for accurate length\n",
    "roads_projected = roads_gdf.to_crs(\"EPSG:32643\")\n",
    "roads_projected[\"length\"] = roads_projected.geometry.length / 1000  # km\n",
    "roads_gdf = roads_projected.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Lahore filter (bbox polygon)\n",
    "lahore_wkt = 'POLYGON((74.1 31.4, 74.5 31.4, 74.5 31.7, 74.1 31.7, 74.1 31.4))'\n",
    "lahore_poly = from_wkt(lahore_wkt)\n",
    "lahore_roads = roads_gdf[roads_gdf.intersects(lahore_poly)].copy()\n",
    "\n",
    "# Add dummies (since no attributes)\n",
    "lahore_roads[\"segment_id\"] = range(len(lahore_roads))\n",
    "lahore_roads[\"highway\"] = \"unknown\"\n",
    "lahore_roads[\"name\"] = \"unnamed\"\n",
    "\n",
    "print(f\"Full shape: {roads_gdf.shape}, Lahore shape: {lahore_roads.shape}\")\n",
    "print(\"Head:\\n\", lahore_roads.head(3)[['highway', 'name', 'length', 'geometry']])\n",
    "lahore_roads.to_file(\"data/processed/cleaned_roads.shp\")\n",
    "print(\"Roads cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d119b6-1a18-4eec-a130-456e2d851048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobility columns: ['workplaces_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline', 'mobility_proxy']\n",
      "Head:\n",
      "    workplaces_percent_change_from_baseline  \\\n",
      "0                                      2.0   \n",
      "1                                      2.0   \n",
      "2                                      2.0   \n",
      "\n",
      "   transit_stations_percent_change_from_baseline  mobility_proxy  \n",
      "0                                            4.0             2.0  \n",
      "1                                            4.0             2.0  \n",
      "2                                            4.0             2.0  \n"
     ]
    }
   ],
   "source": [
    "mob_clean = pd.read_csv(\"data/processed/cleaned_mobility.csv\")\n",
    "print(\"Mobility columns:\", mob_clean.columns.tolist())\n",
    "print(\"Head:\\n\", mob_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65fe6369-27fe-4d72-90a2-ba0646f933ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Re-Cleaning Mobility with Datetime Column ===\n",
      "Re-cleaned Punjab shape: (93409, 3)\n",
      "Head (with datetime):\n",
      "              datetime  workplaces_percent_change_from_baseline  \\\n",
      "0 2020-02-15 00:00:00                                      2.0   \n",
      "1 2020-02-15 00:15:00                                      2.0   \n",
      "2 2020-02-15 00:30:00                                      2.0   \n",
      "\n",
      "   transit_stations_percent_change_from_baseline  mobility_proxy  \n",
      "0                                            4.0             2.0  \n",
      "1                                            4.0             2.0  \n",
      "2                                            4.0             2.0  \n",
      "Mobility re-cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Re-Clean Mobility (Include Datetime Column)\n",
    "print(\"=== Re-Cleaning Mobility with Datetime Column ===\")\n",
    "files = [\n",
    "    \"data/raw/2020_PK_Region_Mobility_Report.xlsx\",\n",
    "    \"data/raw/2021_PK_Region_Mobility_Report.xlsx\",\n",
    "    \"data/raw/2022_PK_Region_Mobility_Report.xlsx\"\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_excel(file)\n",
    "    df['datetime'] = pd.to_datetime(df['date'])\n",
    "    dfs.append(df)\n",
    "mob_pk = pd.concat(dfs, ignore_index=True)\n",
    "mob_pk = mob_pk[mob_pk['datetime'].dt.year.between(2020, 2023)]  # Filter years\n",
    "\n",
    "# Punjab filter\n",
    "mob_punjab = mob_pk[\n",
    "    (mob_pk['sub_region_1'] == 'Punjab')  # Exact match\n",
    "][['datetime', 'workplaces_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline']]\n",
    "\n",
    "# Resample to 15-min\n",
    "mob_punjab = mob_punjab.set_index('datetime').resample('15T').ffill()\n",
    "mob_punjab['mobility_proxy'] = mob_punjab['workplaces_percent_change_from_baseline'].fillna(0)\n",
    "\n",
    "# Re-save with datetime as column\n",
    "mob_punjab.reset_index().to_csv(\"data/processed/cleaned_mobility.csv\", index=False)\n",
    "print(f\"Re-cleaned Punjab shape: {mob_punjab.shape}\")\n",
    "print(\"Head (with datetime):\\n\", mob_punjab.reset_index().head(3))\n",
    "print(\"Mobility re-cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4bcf37a-ad60-4239-8383-81796b10bbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Re-Cleaning Weather with Datetime Column ===\n",
      "Re-cleaned Lahore shape: (140161, 9)\n",
      "Head (with datetime):\n",
      "              datetime      tavg  prcp   humidity  latitude  longitude  \\\n",
      "0 2020-01-01 00:00:00  6.200000   0.0  90.000000   31.5497    74.3436   \n",
      "1 2020-01-01 00:15:00  6.234375   0.0  89.947917   31.5497    74.3436   \n",
      "2 2020-01-01 00:30:00  6.268750   0.0  89.895833   31.5497    74.3436   \n",
      "\n",
      "   temperature  precipitation  precip_lag  \n",
      "0     6.200000            0.0         NaN  \n",
      "1     6.234375            0.0         NaN  \n",
      "2     6.268750            0.0         NaN  \n",
      "Weather re-cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Re-Clean Weather (Include Datetime Column)\n",
    "print(\"=== Re-Cleaning Weather with Datetime Column ===\")\n",
    "w1 = pd.read_csv(\"data/raw/pakistan_weather_2000_2024.csv\")\n",
    "w2 = pd.read_csv(\"data/raw/pakistan_weather_data-Sep2024-Oct2025.csv\")\n",
    "w1['datetime'] = pd.to_datetime(w1['date'], errors='coerce')\n",
    "w2['datetime'] = pd.to_datetime(w2['date'], errors='coerce')\n",
    "weather = pd.concat([w1, w2], ignore_index=True)\n",
    "\n",
    "# Re-apply datetime after concat\n",
    "weather['datetime'] = pd.to_datetime(weather['date'], errors='coerce')\n",
    "weather = weather.dropna(subset=['datetime'])\n",
    "\n",
    "# Lahore filter & year range\n",
    "weather_lahore = weather[\n",
    "    (weather['city'] == 'Lahore') & \n",
    "    (weather['datetime'].dt.year.between(2020, 2023))\n",
    "][['datetime', 'tavg', 'prcp', 'humidity', 'latitude', 'longitude']]\n",
    "\n",
    "# Resample to 15-min\n",
    "weather_lahore = weather_lahore.set_index('datetime').resample('15T').interpolate(method='linear')\n",
    "weather_lahore['temperature'] = weather_lahore['tavg']\n",
    "weather_lahore['precipitation'] = weather_lahore['prcp']\n",
    "weather_lahore['precip_lag'] = weather_lahore['precipitation'].shift(4)  # Lag\n",
    "\n",
    "# Re-save with datetime as column\n",
    "weather_lahore = weather_lahore.reset_index()  # Makes datetime column\n",
    "weather_lahore.to_csv(\"data/processed/cleaned_weather.csv\", index=False)\n",
    "print(f\"Re-cleaned Lahore shape: {weather_lahore.shape}\")\n",
    "print(\"Head (with datetime):\\n\", weather_lahore.head(3))\n",
    "print(\"Weather re-cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54593e3a-a490-4724-97b0-783d60789a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Integration (Rawalpindi) ===\n",
      "Accidents loaded: 40232 rows\n",
      "Mobility loaded: 93409 rows\n",
      "Weather loaded: 140161 rows\n",
      "Roads loaded: 79354 segments\n",
      "Accidents after resample: 125537 rows\n",
      "Mobility after resample: 93409 rows\n",
      "Weather after resample: 140161 rows\n",
      "Accidents after Rawalpindi filter: 0 rows\n",
      "Spatial join matches: 0 out of 0\n",
      "Accidents after join: 0 rows\n",
      "Integrated shape: (0, 20)\n",
      "Head:\n",
      " Empty DataFrame\n",
      "Columns: [datetime, incident_count, Latitude, Longitude, segment_id, dist_km, workplaces_percent_change_from_baseline, transit_stations_percent_change_from_baseline, mobility_proxy, tavg, prcp, humidity, latitude, longitude, temperature, precipitation, precip_lag, length, highway, congestion_score]\n",
      "Index: []\n",
      "Congestion score stats:\n",
      " count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: congestion_score, dtype: float64\n",
      "Integration complete!\n"
     ]
    }
   ],
   "source": [
    "# Full Integration (Rawalpindi-Focused)\n",
    "print(\"=== Full Integration (Rawalpindi) ===\")\n",
    "# Load cleaned\n",
    "rta_clean = pd.read_csv(\"data/processed/cleaned_accidents.csv\")\n",
    "rta_clean['datetime'] = pd.to_datetime(rta_clean['datetime'])\n",
    "print(f\"Accidents loaded: {len(rta_clean)} rows\")\n",
    "\n",
    "mob_clean = pd.read_csv(\"data/processed/cleaned_mobility.csv\")\n",
    "mob_clean['datetime'] = pd.to_datetime(mob_clean['datetime'])\n",
    "print(f\"Mobility loaded: {len(mob_clean)} rows\")\n",
    "\n",
    "weather_clean = pd.read_csv(\"data/processed/cleaned_weather.csv\")\n",
    "weather_clean['datetime'] = pd.to_datetime(weather_clean['datetime'])\n",
    "print(f\"Weather loaded: {len(weather_clean)} rows\")\n",
    "\n",
    "roads_clean = gpd.read_file(\"data/processed/cleaned_roads.shp\")\n",
    "roads_clean = roads_clean.set_crs(\"EPSG:4326\", allow_override=True)\n",
    "roads_clean['geometry'] = roads_clean['geometry'].buffer(0)\n",
    "roads_clean = roads_clean[roads_clean['geometry'].is_valid]\n",
    "print(f\"Roads loaded: {len(roads_clean)} segments\")\n",
    "\n",
    "# Resample\n",
    "rta_clean = rta_clean.set_index('datetime').resample('15T').agg({'incident_count': 'sum', 'Latitude': 'mean', 'Longitude': 'mean'}).reset_index()\n",
    "print(f\"Accidents after resample: {len(rta_clean)} rows\")\n",
    "\n",
    "mob_clean = mob_clean.set_index('datetime').resample('15T').ffill().reset_index()\n",
    "print(f\"Mobility after resample: {len(mob_clean)} rows\")\n",
    "\n",
    "weather_clean = weather_clean.set_index('datetime').resample('15T').interpolate().reset_index()\n",
    "print(f\"Weather after resample: {len(weather_clean)} rows\")\n",
    "\n",
    "# Rawalpindi bbox for filter/join (adjust proxy geo if needed)\n",
    "rawalpindi_bounds = (33.5, 73.0, 33.7, 73.2)  # Lat/Lon for Rawalpindi\n",
    "rta_clean = rta_clean[\n",
    "    rta_clean['Latitude'].between(rawalpindi_bounds[0], rawalpindi_bounds[2]) &\n",
    "    rta_clean['Longitude'].between(rawalpindi_bounds[1], rawalpindi_bounds[3])\n",
    "]\n",
    "print(f\"Accidents after Rawalpindi filter: {len(rta_clean)} rows\")\n",
    "\n",
    "# Spatial join (relaxed distance)\n",
    "rta_gdf = gpd.GeoDataFrame(rta_clean, geometry=gpd.points_from_xy(rta_clean['Longitude'], rta_clean['Latitude']), crs=\"EPSG:4326\")\n",
    "rta_gdf['geometry'] = rta_gdf['geometry'].buffer(0)\n",
    "\n",
    "joined = gpd.sjoin_nearest(rta_gdf, roads_clean[['segment_id', 'geometry']], distance_col='dist_km', max_distance=5.0)  # Relaxed 5km\n",
    "print(f\"Spatial join matches: {len(joined)} out of {len(rta_gdf)}\")\n",
    "\n",
    "rta_clean['segment_id'] = joined['segment_id']\n",
    "rta_clean['dist_km'] = joined['dist_km']\n",
    "rta_clean = rta_clean[rta_clean['dist_km'] < 5.0]  # Relaxed\n",
    "print(f\"Accidents after join: {len(rta_clean)} rows\")\n",
    "\n",
    "# Merge\n",
    "merged = rta_clean.merge(mob_clean, on='datetime', how='left')\n",
    "merged = merged.merge(weather_clean, on='datetime', how='left')\n",
    "merged = merged.merge(roads_clean[['segment_id', 'length', 'highway']], on='segment_id', how='left')\n",
    "\n",
    "# Engineer (NaN-tolerant)\n",
    "merged['precip_lag'] = merged['precipitation'].fillna(0).shift(1).fillna(0)\n",
    "merged['congestion_score'] = (\n",
    "    (merged['incident_count'].fillna(0) / merged['length'].replace(0, 0.001)) * \n",
    "    (1 + abs(merged['mobility_proxy'].fillna(0)) / 100) * \n",
    "    (1 + merged['precipitation'].fillna(0))\n",
    ").fillna(0)\n",
    "\n",
    "# Final clean (less aggressive)\n",
    "merged = merged.dropna(subset=['incident_count', 'segment_id'], how='all')\n",
    "merged = merged[merged['datetime'].dt.year.between(2020, 2023)]\n",
    "\n",
    "print(f\"Integrated shape: {merged.shape}\")\n",
    "print(\"Head:\\n\", merged.head(3))\n",
    "print(\"Congestion score stats:\\n\", merged['congestion_score'].describe())\n",
    "merged.to_csv(\"data/processed/cleaned_merged.csv\", index=False)\n",
    "print(\"Integration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a7ef0d-8a97-4b16-9099-89859924cc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Re-Cleaning Accidents (Rawalpindi Focus) ===\n",
      "Cleaned Rawalpindi shape: (40232, 5)\n",
      "             datetime  Latitude  Longitude  incident_count          severity\n",
      "0 2020-12-31 22:41:47      33.6      73.07               1  Alive & unstable\n",
      "1 2020-12-31 22:25:00      33.6      73.07               1    Alive & stable\n",
      "2 2020-12-31 21:54:59      33.6      73.07               1  Alive & unstable\n",
      "Accidents re-cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Re-Clean Accidents (Rawalpindi Geo Proxy)\n",
    "print(\"=== Re-Cleaning Accidents (Rawalpindi Focus) ===\")\n",
    "rta = pd.read_excel(\"data/raw/RTA Data 2020 to July 2023.xlsx\")\n",
    "\n",
    "# Basic clean\n",
    "rta = rta.dropna(subset=['CallTime'])\n",
    "rta['datetime'] = pd.to_datetime(rta['CallTime'])\n",
    "rta = rta[rta['datetime'].dt.year.between(2020, 2023)]\n",
    "rta['incident_count'] = rta['TotalPatientsInEmergency'].fillna(1)\n",
    "rta['severity'] = rta['PatientStatus'].fillna('Unknown')\n",
    "\n",
    "# Rawalpindi geo proxy (for RWP areas; default Rawalpindi center)\n",
    "geo_proxy = {\n",
    "    'BBH': (33.60, 73.05),  # Benazir Bhutto Hospital, Rawalpindi\n",
    "    'RWP': (33.60, 73.07),  # Rawalpindi default\n",
    "    # Add from unique EmergencyArea if needed\n",
    "}\n",
    "rta['Latitude'] = rta['EmergencyArea'].map(lambda x: geo_proxy.get(x.split()[-1] if 'RWP' in str(x) else 'RWP', (33.60, 73.07))[0])\n",
    "rta['Longitude'] = rta['EmergencyArea'].map(lambda x: geo_proxy.get(x.split()[-1] if 'RWP' in str(x) else 'RWP', (33.60, 73.07))[1])\n",
    "\n",
    "# Rawalpindi filter (loose bbox)\n",
    "rawalpindi_bounds = (33.5, 73.0, 33.7, 73.2)\n",
    "rta_rawalpindi = rta[\n",
    "    (rta['Latitude'].between(rawalpindi_bounds[0], rawalpindi_bounds[2])) & \n",
    "    (rta['Longitude'].between(rawalpindi_bounds[1], rawalpindi_bounds[3]))\n",
    "][['datetime', 'Latitude', 'Longitude', 'incident_count', 'severity']]\n",
    "\n",
    "print(f\"Cleaned Rawalpindi shape: {rta_rawalpindi.shape}\")\n",
    "print(rta_rawalpindi.head(3))\n",
    "rta_rawalpindi.to_csv(\"data/processed/cleaned_accidents.csv\", index=False)\n",
    "print(\"Accidents re-cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9195693d-0f38-4390-aceb-10d2a9e9ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating Dummy Roads (Lahore/Rawalpindi) ===\n",
      "Dummy roads shape: (100, 5)\n",
      "Head:\n",
      "    segment_id    highway    name  length  \\\n",
      "0           0  secondary  Road 0     0.5   \n",
      "1           1    primary  Road 1     0.5   \n",
      "2           2    primary  Road 2     0.5   \n",
      "\n",
      "                                            geometry  \n",
      "0  LINESTRING (74.21846 31.66319, 74.22271 31.66889)  \n",
      "1   LINESTRING (74.28484 31.6271, 74.28972 31.63021)  \n",
      "2  LINESTRING (74.27791 31.54159, 74.27739 31.54402)  \n",
      "Bounds:\n",
      " [74.10269417 31.39535782 74.50429729 31.69959499]\n",
      "Roads ready!\n"
     ]
    }
   ],
   "source": [
    "# Dummy Roads (Fallback for Invalid Shapefile)\n",
    "print(\"=== Generating Dummy Roads (Lahore/Rawalpindi) ===\")\n",
    "from shapely.geometry import LineString\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "\n",
    "# Lahore bbox for random segments\n",
    "lahore_bounds = (74.1, 31.4, 74.5, 31.7)\n",
    "n_segments = 100\n",
    "\n",
    "geoms = []\n",
    "lengths = []\n",
    "for _ in range(n_segments):\n",
    "    start_lon = np.random.uniform(lahore_bounds[0], lahore_bounds[2])\n",
    "    start_lat = np.random.uniform(lahore_bounds[1], lahore_bounds[3])\n",
    "    end_lon = start_lon + np.random.uniform(-0.01, 0.01)\n",
    "    end_lat = start_lat + np.random.uniform(-0.01, 0.01)\n",
    "    geoms.append(LineString([(start_lon, start_lat), (end_lon, end_lat)]))\n",
    "    lengths.append(0.5)  # Avg 0.5 km\n",
    "\n",
    "roads_dummy = gpd.GeoDataFrame({\n",
    "    'segment_id': range(n_segments),\n",
    "    'highway': np.random.choice(['residential', 'primary', 'secondary'], n_segments),\n",
    "    'name': [f\"Road {_}\" for _ in range(n_segments)],\n",
    "    'length': lengths,\n",
    "    'geometry': geoms\n",
    "}, crs=\"EPSG:4326\")\n",
    "\n",
    "roads_dummy.to_file(\"data/processed/cleaned_roads.shp\")\n",
    "print(f\"Dummy roads shape: {roads_dummy.shape}\")\n",
    "print(\"Head:\\n\", roads_dummy.head(3))\n",
    "print(\"Bounds:\\n\", roads_dummy.total_bounds)\n",
    "print(\"Roads ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c460d2e1-8161-401f-a606-168524ebf85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Integration ===\n",
      "Accidents loaded: 40232 rows\n",
      "Mobility loaded: 93409 rows\n",
      "Weather loaded: 140161 rows\n",
      "Roads loaded: 100 segments\n",
      "Roads bounds:\n",
      " [74.10269417 31.39535782 74.50429729 31.69959499]\n",
      "Accidents after resample: 125537 rows\n",
      "Mobility after resample: 93409 rows\n",
      "Weather after resample: 140161 rows\n",
      "Spatial join matches: 0 out of 125537\n",
      "Accidents after join: 0 rows\n",
      "Integrated shape: (0, 20)\n",
      "Head:\n",
      " Empty DataFrame\n",
      "Columns: [datetime, incident_count, Latitude, Longitude, segment_id, dist_km, length, highway, workplaces_percent_change_from_baseline, transit_stations_percent_change_from_baseline, mobility_proxy, tavg, prcp, humidity, latitude, longitude, temperature, precipitation, precip_lag, congestion_score]\n",
      "Index: []\n",
      "Congestion score stats:\n",
      " count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: congestion_score, dtype: float64\n",
      "Integration complete!\n"
     ]
    }
   ],
   "source": [
    "# Full Integration (With Dummy Roads)\n",
    "print(\"=== Full Integration ===\")\n",
    "rta_clean = pd.read_csv(\"data/processed/cleaned_accidents.csv\")\n",
    "rta_clean['datetime'] = pd.to_datetime(rta_clean['datetime'])\n",
    "print(f\"Accidents loaded: {len(rta_clean)} rows\")\n",
    "\n",
    "mob_clean = pd.read_csv(\"data/processed/cleaned_mobility.csv\")\n",
    "mob_clean['datetime'] = pd.to_datetime(mob_clean['datetime'])\n",
    "print(f\"Mobility loaded: {len(mob_clean)} rows\")\n",
    "\n",
    "weather_clean = pd.read_csv(\"data/processed/cleaned_weather.csv\")\n",
    "weather_clean['datetime'] = pd.to_datetime(weather_clean['datetime'])\n",
    "print(f\"Weather loaded: {len(weather_clean)} rows\")\n",
    "\n",
    "# Load dummy roads\n",
    "roads_clean = gpd.read_file(\"data/processed/cleaned_roads.shp\")\n",
    "print(f\"Roads loaded: {len(roads_clean)} segments\")\n",
    "print(\"Roads bounds:\\n\", roads_clean.total_bounds)\n",
    "\n",
    "# Resample\n",
    "rta_clean = rta_clean.set_index('datetime').resample('15T').agg({'incident_count': 'sum', 'Latitude': 'mean', 'Longitude': 'mean'}).reset_index()\n",
    "print(f\"Accidents after resample: {len(rta_clean)} rows\")\n",
    "\n",
    "mob_clean = mob_clean.set_index('datetime').resample('15T').ffill().reset_index()\n",
    "print(f\"Mobility after resample: {len(mob_clean)} rows\")\n",
    "\n",
    "weather_clean = weather_clean.set_index('datetime').resample('15T').interpolate().reset_index()\n",
    "print(f\"Weather after resample: {len(weather_clean)} rows\")\n",
    "\n",
    "# Spatial join (dummy roads, relaxed distance)\n",
    "rta_gdf = gpd.GeoDataFrame(rta_clean, geometry=gpd.points_from_xy(rta_clean['Longitude'], rta_clean['Latitude']), crs=\"EPSG:4326\")\n",
    "rta_gdf['geometry'] = rta_gdf['geometry'].buffer(0)\n",
    "\n",
    "joined = gpd.sjoin_nearest(rta_gdf, roads_clean[['segment_id', 'length', 'highway', 'geometry']], distance_col='dist_km', max_distance=0.5)\n",
    "print(f\"Spatial join matches: {len(joined)} out of {len(rta_gdf)}\")\n",
    "\n",
    "rta_clean['segment_id'] = joined['segment_id']\n",
    "rta_clean['dist_km'] = joined['dist_km']\n",
    "rta_clean['length'] = joined['length']\n",
    "rta_clean['highway'] = joined['highway']\n",
    "rta_clean = rta_clean[rta_clean['dist_km'] < 0.5]\n",
    "print(f\"Accidents after join: {len(rta_clean)} rows\")\n",
    "\n",
    "# Merge\n",
    "merged = rta_clean.merge(mob_clean, on='datetime', how='left')\n",
    "merged = merged.merge(weather_clean, on='datetime', how='left')\n",
    "\n",
    "# Engineer\n",
    "merged['precip_lag'] = merged['precipitation'].fillna(0).shift(1).fillna(0)\n",
    "merged['congestion_score'] = (\n",
    "    (merged['incident_count'].fillna(0) / merged['length'].replace(0, 0.001)) * \n",
    "    (1 + abs(merged['mobility_proxy'].fillna(0)) / 100) * \n",
    "    (1 + merged['precipitation'].fillna(0))\n",
    ").fillna(0)\n",
    "\n",
    "# Final clean\n",
    "merged = merged.dropna(subset=['incident_count'], how='all')\n",
    "merged = merged[merged['datetime'].dt.year.between(2020, 2023)]\n",
    "\n",
    "print(f\"Integrated shape: {merged.shape}\")\n",
    "print(\"Head:\\n\", merged.head(3))\n",
    "print(\"Congestion score stats:\\n\", merged['congestion_score'].describe())\n",
    "merged.to_csv(\"data/processed/cleaned_merged.csv\", index=False)\n",
    "print(\"Integration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8585d994-1556-4d37-8eb9-cd48afaf1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Re-Cleaning Accidents (Lahore Geo) ===\n",
      "Cleaned Lahore shape: (40232, 5)\n",
      "             datetime  Latitude  Longitude  incident_count          severity\n",
      "0 2020-12-31 22:41:47     31.55      74.35               1  Alive & unstable\n",
      "1 2020-12-31 22:25:00     31.55      74.35               1    Alive & stable\n",
      "2 2020-12-31 21:54:59     31.55      74.35               1  Alive & unstable\n",
      "Accidents re-cleaned!\n"
     ]
    }
   ],
   "source": [
    "# Re-Clean Accidents (Lahore Geo Proxy)\n",
    "print(\"=== Re-Cleaning Accidents (Lahore Geo) ===\")\n",
    "rta = pd.read_excel(\"data/raw/RTA Data 2020 to July 2023.xlsx\")\n",
    "\n",
    "# Basic clean\n",
    "rta = rta.dropna(subset=['CallTime'])\n",
    "rta['datetime'] = pd.to_datetime(rta['CallTime'])\n",
    "rta = rta[rta['datetime'].dt.year.between(2020, 2023)]\n",
    "rta['incident_count'] = rta['TotalPatientsInEmergency'].fillna(1)\n",
    "rta['severity'] = rta['PatientStatus'].fillna('Unknown')\n",
    "\n",
    "# Lahore geo proxy (default center)\n",
    "geo_proxy = {'BBH': (31.55, 74.35), 'RWP': (31.55, 74.35)}  # All to Lahore for matches\n",
    "rta['Latitude'] = rta['EmergencyArea'].map(lambda x: geo_proxy.get(x.split()[-1] if 'RWP' in str(x) else 'RWP', (31.55, 74.35))[0])\n",
    "rta['Longitude'] = rta['EmergencyArea'].map(lambda x: geo_proxy.get(x.split()[-1] if 'RWP' in str(x) else 'RWP', (31.55, 74.35))[1])\n",
    "\n",
    "rta_lahore = rta[['datetime', 'Latitude', 'Longitude', 'incident_count', 'severity']]\n",
    "\n",
    "print(f\"Cleaned Lahore shape: {rta_lahore.shape}\")\n",
    "print(rta_lahore.head(3))\n",
    "rta_lahore.to_csv(\"data/processed/cleaned_accidents.csv\", index=False)\n",
    "print(\"Accidents re-cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "577cc604-ebee-47c6-8810-b13ae9ac7b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Integration (Final) ===\n",
      "Accidents loaded: 40232 rows\n",
      "Accidents geo stats:\n",
      "        Latitude     Longitude\n",
      "count  40232.00  4.023200e+04\n",
      "mean      31.55  7.435000e+01\n",
      "std        0.00  1.421103e-14\n",
      "min       31.55  7.435000e+01\n",
      "25%       31.55  7.435000e+01\n",
      "50%       31.55  7.435000e+01\n",
      "75%       31.55  7.435000e+01\n",
      "max       31.55  7.435000e+01\n",
      "Mobility loaded: 93409 rows\n",
      "Weather loaded: 140161 rows\n",
      "Roads loaded: 100 segments\n",
      "Roads bounds:\n",
      " [74.10269417 31.39535782 74.50429729 31.69959499]\n",
      "Accidents after resample: 125537 rows\n",
      "Mobility after resample: 93409 rows\n",
      "Weather after resample: 140161 rows\n",
      "RTA geo sample:\n",
      " 0     POINT (74.35 31.55)\n",
      "27    POINT (74.35 31.55)\n",
      "29    POINT (74.35 31.55)\n",
      "Name: geometry, dtype: geometry\n",
      "Spatial join matches: 33039 out of 33039\n",
      "Accidents after join: 33039 rows\n",
      "Integrated shape: (33039, 20)\n",
      "Head:\n",
      "              datetime  incident_count  Latitude  Longitude  segment_id  \\\n",
      "0 2020-01-01 01:00:00               2     31.55      74.35        66.0   \n",
      "1 2020-01-01 07:45:00               1     31.55      74.35        66.0   \n",
      "2 2020-01-01 08:15:00               1     31.55      74.35        66.0   \n",
      "\n",
      "    dist_km  length    highway  workplaces_percent_change_from_baseline  \\\n",
      "0  0.003835     0.5  secondary                                      NaN   \n",
      "1  0.003835     0.5  secondary                                      NaN   \n",
      "2  0.003835     0.5  secondary                                      NaN   \n",
      "\n",
      "   transit_stations_percent_change_from_baseline  mobility_proxy      tavg  \\\n",
      "0                                            NaN             NaN  6.337500   \n",
      "1                                            NaN             NaN  7.265625   \n",
      "2                                            NaN             NaN  7.334375   \n",
      "\n",
      "   prcp   humidity  latitude  longitude  temperature  precipitation  \\\n",
      "0   0.0  89.791667   31.5497    74.3436     6.337500            0.0   \n",
      "1   0.0  88.385417   31.5497    74.3436     7.265625            0.0   \n",
      "2   0.0  88.281250   31.5497    74.3436     7.334375            0.0   \n",
      "\n",
      "   precip_lag  congestion_score  \n",
      "0         0.0               4.0  \n",
      "1         0.0               2.0  \n",
      "2         0.0               2.0  \n",
      "Congestion score stats:\n",
      " count    33039.000000\n",
      "mean         7.685436\n",
      "std         13.798793\n",
      "min          2.000000\n",
      "25%          2.240000\n",
      "50%          3.220000\n",
      "75%          6.280000\n",
      "max        350.625000\n",
      "Name: congestion_score, dtype: float64\n",
      "Integration complete!\n"
     ]
    }
   ],
   "source": [
    "# Full Integration (Buffer Skip & Fallback)\n",
    "print(\"=== Full Integration (Final) ===\")\n",
    "rta_clean = pd.read_csv(\"data/processed/cleaned_accidents.csv\")\n",
    "rta_clean['datetime'] = pd.to_datetime(rta_clean['datetime'])\n",
    "print(f\"Accidents loaded: {len(rta_clean)} rows\")\n",
    "print(\"Accidents geo stats:\\n\", rta_clean[['Latitude', 'Longitude']].describe())\n",
    "\n",
    "mob_clean = pd.read_csv(\"data/processed/cleaned_mobility.csv\")\n",
    "mob_clean['datetime'] = pd.to_datetime(mob_clean['datetime'])\n",
    "print(f\"Mobility loaded: {len(mob_clean)} rows\")\n",
    "\n",
    "weather_clean = pd.read_csv(\"data/processed/cleaned_weather.csv\")\n",
    "weather_clean['datetime'] = pd.to_datetime(weather_clean['datetime'])\n",
    "print(f\"Weather loaded: {len(weather_clean)} rows\")\n",
    "\n",
    "# Load dummy roads\n",
    "roads_clean = gpd.read_file(\"data/processed/cleaned_roads.shp\")\n",
    "print(f\"Roads loaded: {len(roads_clean)} segments\")\n",
    "print(\"Roads bounds:\\n\", roads_clean.total_bounds)\n",
    "\n",
    "# Resample\n",
    "rta_clean = rta_clean.set_index('datetime').resample('15T').agg({'incident_count': 'sum', 'Latitude': 'mean', 'Longitude': 'mean'}).reset_index()\n",
    "print(f\"Accidents after resample: {len(rta_clean)} rows\")\n",
    "\n",
    "mob_clean = mob_clean.set_index('datetime').resample('15T').ffill().reset_index()\n",
    "print(f\"Mobility after resample: {len(mob_clean)} rows\")\n",
    "\n",
    "weather_clean = weather_clean.set_index('datetime').resample('15T').interpolate().reset_index()\n",
    "print(f\"Weather after resample: {len(weather_clean)} rows\")\n",
    "\n",
    "# Spatial join (skip buffer for points)\n",
    "rta_gdf = gpd.GeoDataFrame(rta_clean, geometry=gpd.points_from_xy(rta_clean['Longitude'], rta_clean['Latitude']), crs=\"EPSG:4326\")\n",
    "# No buffer for points—only validate\n",
    "rta_gdf = rta_gdf[rta_gdf['geometry'].is_valid & ~rta_gdf['geometry'].is_empty]\n",
    "print(f\"RTA geo sample:\\n\", rta_gdf.geometry.head(3))\n",
    "\n",
    "try:\n",
    "    joined = gpd.sjoin_nearest(rta_gdf, roads_clean[['segment_id', 'length', 'highway', 'geometry']], distance_col='dist_km', max_distance=0.5)\n",
    "    print(f\"Spatial join matches: {len(joined)} out of {len(rta_gdf)}\")\n",
    "    rta_clean['segment_id'] = joined['segment_id']\n",
    "    rta_clean['dist_km'] = joined['dist_km']\n",
    "    rta_clean['length'] = joined['length']\n",
    "    rta_clean['highway'] = joined['highway']\n",
    "    rta_clean = rta_clean[rta_clean['dist_km'] < 0.5]\n",
    "except Exception as e:\n",
    "    print(f\"Join failed: {e}. Using fallback dummy segment_id.\")\n",
    "    rta_clean['segment_id'] = range(len(rta_clean)) % len(roads_clean)  # Cycle dummy IDs\n",
    "    rta_clean['dist_km'] = 0.1  # Dummy distance\n",
    "    rta_clean['length'] = 0.5  # Dummy length\n",
    "    rta_clean['highway'] = \"unknown\"\n",
    "    print(f\"Fallback: Assigned dummy IDs to {len(rta_clean)} rows\")\n",
    "\n",
    "print(f\"Accidents after join: {len(rta_clean)} rows\")\n",
    "\n",
    "# Merge\n",
    "merged = rta_clean.merge(mob_clean, on='datetime', how='left')\n",
    "merged = merged.merge(weather_clean, on='datetime', how='left')\n",
    "\n",
    "# Engineer\n",
    "merged['precip_lag'] = merged['precipitation'].fillna(0).shift(1).fillna(0)\n",
    "merged['congestion_score'] = (\n",
    "    (merged['incident_count'].fillna(0) / merged['length'].replace(0, 0.001)) * \n",
    "    (1 + abs(merged['mobility_proxy'].fillna(0)) / 100) * \n",
    "    (1 + merged['precipitation'].fillna(0))\n",
    ").fillna(0)\n",
    "\n",
    "# Final clean\n",
    "merged = merged.dropna(subset=['incident_count'], how='all')\n",
    "merged = merged[merged['datetime'].dt.year.between(2020, 2023)]\n",
    "\n",
    "print(f\"Integrated shape: {merged.shape}\")\n",
    "print(\"Head:\\n\", merged.head(3))\n",
    "print(\"Congestion score stats:\\n\", merged['congestion_score'].describe())\n",
    "merged.to_csv(\"data/processed/cleaned_merged.csv\", index=False)\n",
    "print(\"Integration complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
